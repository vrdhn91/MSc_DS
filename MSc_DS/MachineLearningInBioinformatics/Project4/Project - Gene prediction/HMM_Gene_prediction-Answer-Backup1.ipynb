{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gene Prediction with Hidden Markov Models\n",
    "\n",
    "\n",
    "Hidden Markov Models (HMM) are widely used in various fields of research: speech recognition, automatic natural language processing, handwriting recognition, and bioinformatics.\n",
    "\n",
    "The 3 main problems associated to HMMs are:\n",
    "\n",
    "  1. Evaluation :\n",
    "    - Problem: Compute the probability of observing the sequence given an HMM:\n",
    "    - Solution: **Forward Algorithm**\n",
    "\n",
    "  2. Decoding:\n",
    "    - Problem: find the sequence of states that maximizes the probability of observing the sequences.\n",
    "    - Solution: **Viterbi Algorithm**\n",
    "\n",
    "  3. Training/Estimation:\n",
    "    - Problem: Adjust the parameters of the HMM model to maximize the probability of generating the sequence of observations from the training data\n",
    "    - Solution: **Forward-Backward Algorithm**\n",
    "\n",
    "In this TME, we will apply the Viterbi algorithm to molecular biology data, in particular for the problem of gene prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Biological background \n",
    "\n",
    "In this small project, we will see how statistical models can be used to extract information from raw biological data. The goal will be to specify Hidden Markov Models which will allow to annotate the positions of the genes in the genome.\n",
    "\n",
    "The genome, the carrier of genetic information, can be thought of as a long sequence of characters written in a 4-letter alphabet: `A`, `C`, `G` and `T`. Each letter of the genome is also called a base pair (or bp). It is now relatively inexpensive to sequence a genome (some direct to consumer company have [offers](https://nebula.org/whole-genome-sequencing-dna-test/) as low as a few hundred euros for a human genome). However, we cannot understand, simply from the series of letters, how this information is used by the cell (a bit like having an instruction manual written in an unknown language, or a compiled code with no information on the machine).\n",
    "\n",
    "An essential element is the gene, which after transcription and translation will produce proteins, the molecules responsible for much of the biochemical activity of cells.\n",
    "\n",
    "_if you do not know about transcription and translation, a short video about the basics:_ \n",
    "https://youtube.com/shorts/mSMjwxNK2EU?feature=share\n",
    "\n",
    "Ameoba sisters page\n",
    "https://www.youtube.com/c/AmoebaSisters\n",
    "\n",
    "\n",
    "## Two paragraphs summary for the impatient:\n",
    "\n",
    "The translation into protein is done using the genetic code which, for each group of 3 letters (or bp) transcribed, matches an amino acid. These groups of 3 letters are called codons and there are $ 4 ^ 3 $, or $ 64 $. So, as a first approximation, a gene is defined by the following properties (for prokaryotic organisms):\n",
    "\n",
    "- The first codon, called start codon is `ATG`,\n",
    "- There are 61 codons which code for Amino Acids.\n",
    "- The last codon, called the stop codon, marks the end of the gene and is one of the three sequences `TAA`, `TAG` or `TGA`. It does not appear in the gene.\n",
    "\n",
    "\n",
    "We will integrate these different pieces of information to predict the positions of genes. Note that this figure is for the moment simplified, as we have omitted the fact that the DNA molecule consists of two complementary strands, and therefore that the genes present on the complementary strand are seen \"upside down\" on our sequence. \n",
    "\n",
    "The regions between genes are simply called *intergenic regions*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image of Yaktocat](https://cdn.kastatic.org/ka-perseus-images/1ade7bbd40ca8dbc7a55ddf4067935e42c347f35.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important information to remember for the following:**  Each gene sequence starts with a start codon and ends with a stop codon "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A few more biological information for the uninitiated\n",
    "\n",
    "What is a chromosome?\n",
    "https://www.youtube.com/watch?v=IePMXxQ-KWY\n",
    "\n",
    "Some more information about DNA and RNA (6min)\n",
    "https://youtu.be/JQByjprj_mA\n",
    "\n",
    "And if you like to know how protein synthesis works (9min): <br>\n",
    "https://youtu.be/oefAI2x2CQM\n",
    "<br>\n",
    "And other video on the subject (3min):\n",
    "https://www.youtube.com/watch?v=gG7uCskUOrA\n",
    "\n",
    "\n",
    "What is a gene (5min)? <br>\n",
    "https://www.youtube.com/watch?v=5MQdXjRPHmQ&list=PLInNVsmlBUlQT_peuWctrmGMiLngK-6fb&index=7\n",
    "\n",
    "Here is a short (6min) video explaining the basis of gene regulation. Note that the part about operon is not important. \n",
    "https://youtu.be/h_1QLdtF8d0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gene Modeling\n",
    "\n",
    "## Question 1: data download\n",
    "\n",
    "\n",
    "We will be working on the first million bp of the E. coli genome (strain 042). Rather than working with the letters A, C, G, and T, we'll recode them with numbers ($ A = $ 0, $ C = $ 1, $ G = $ 2, $ T = $ 3).\n",
    "\n",
    "The annotations provided are also encoded with integer values from $ 0 $ to $ 3 $:\n",
    "- 0: the position is in a non-coding region = intergenic region\n",
    "- 1: the position corresponds to a codon in phase 0\n",
    "- 2: the position corresponds to a codon in phase 1 \n",
    "- 3: the position corresponds to a codon in phase 2\n",
    "\n",
    "For instance for the drawing above we have the following values:\n",
    "  - `32031032031312300203031` for the sequence\n",
    "  - `00000123123123123000000` for the annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pickle files for the genome sequences and \n",
    "# its annotation \n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "\n",
    "Genome=np.load('genome.npy') # the first Mio bp of E. coli\n",
    "Annotation=np.load('annotation.npy')# gene annotation \n",
    "\n",
    "## Let's split the data in two, one half for training and \n",
    "## the second half for testing.\n",
    "\n",
    "genome_train=Genome[:500000]\n",
    "genome_test=Genome[500000:]\n",
    "\n",
    "annotation_train=Annotation[:500000]\n",
    "annotation_test=Annotation[500000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(genome_train[0:5])\n",
    "print(annotation_train[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Parameters Estimation / Learning\n",
    "\n",
    "As the simplest model for separating codon sequences from intergenic sequences, we will define the hidden Markov chain whose transition graph is given below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![modele1](modele1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such a model is defined as follows: we consider that there are 4 possible hidden states (intergenic, codon phase 0, codon phase 1, codon phase 2).\n",
    "\n",
    "You can stay in the intergenic regions, and when you start a gene, the composition of each base of the codon is different. In order to be able to use this model to classify, it will be necessary to know the parameters for the transition matrix (so here only the probas $ a $ and $ b $), and the distribution $ (b_i, i = 0,â€¦, 3 ) $ of the nucleotides given the four states.\n",
    "\n",
    "``` python\n",
    "Pi = np.array ([1, 0, 0, 0]) ## we start in intergenic regions\n",
    "A = np.array ([[1-a, a, 0, 0],\n",
    "               [0, 0, 1, 0],\n",
    "               [0, 0, 0, 1],\n",
    "               [b, 1-b, 0, 0]])\n",
    "B = ...\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the structure of an HMM:\n",
    "\n",
    "- The initial distribution $ \\ Pi $ and the transition matrix $ A $ are estimated in the same way as for a simple Markov model (see lecture 4). In other words, the observations have no influence on the hidden states when they are known.\n",
    "- The distribution of each observation only depends on the current state.\n",
    "\n",
    "Given the nature of the data we use Multinoulli for the emissions. As a convenience we will store all the distributions $ b_i $ in a matrix $ B $ (emission probability matrix) structured as follows:\n",
    "\n",
    "- $ K $ columns (number of possible states), $ N $ rows (number of states)\n",
    "- Each row corresponds to an emission law for a state (ie, each row sums to 1)\n",
    "\n",
    "We can now simply learn the parameters $ b_i $ with the two following steps:\n",
    "\n",
    "1. for each state $ i \\in \\Sigma$ store in cell  $ b_ {i,j} $ the number of times the letter j was observed with state $ i $.\n",
    "2. Normalize the rows of $ B $ to sum to one.\n",
    "\n",
    "Write the code of the function `def learnHMM (allX, allS, N, K):` which learns a model from the combined sequence of observations and sequence of states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learnHMM(allx, allq, N, K):\n",
    "    \"\"\" Learn an HMM given a pair of observation and states \n",
    "    np.array[int] * np.array[int] * int * int -> \n",
    "            (np.array[double,double], np.array[double,double])\n",
    "    return transition matrices A and B\"\"\"\n",
    "    A = np.zeros((N, N)) \n",
    "    B = np.zeros((N, K)) \n",
    "    \n",
    "    ### Your code here\n",
    "    A_temp = np.zeros((N, N)) \n",
    "    for i in range(len(allq)-1):\n",
    "        A_temp[allq[i], allq[i+1]] += 1\n",
    "    A = normalize_matrix(A_temp)\n",
    "    \n",
    "    B_count = np.zeros((N, K)) \n",
    "    for i, val_i in enumerate(allx):\n",
    "        val_j = allq[i]\n",
    "        B_count[val_j, val_i] += 1 #K is the ACTG protein (columns). N is the codon dimension (rows)\n",
    "            \n",
    "    B = normalize_matrix(B_count)\n",
    "            \n",
    "    return A,B\n",
    "\n",
    "def normalize_matrix(matrix):\n",
    "    norm_matrix = np.zeros((matrix.shape[0], matrix.shape[1])) \n",
    "    for i, row_i in enumerate(matrix):\n",
    "        if np.sum(row_i) != 0:\n",
    "            norm_matrix[i] = row_i / np.sum(row_i)\n",
    "        else: \n",
    "            norm_matrix[i] = 0.0\n",
    "    return norm_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Pi = np.array([1, 0, 0, 0])\n",
    "nb_states= 4 ## (intergenic, codon 0, codon 1, codon 2)\n",
    "nb_observation = 4 ## (A,C,G,T)\n",
    "A,B =learnHMM(genome_train, annotation_train, nb_states, nb_observation)\n",
    "print(A)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should find:\n",
    "\n",
    "$A= $\n",
    "```python\n",
    "[[0.99899016 0.00100984 0.         0.        ]\n",
    " [0.         0.         1.         0.        ]\n",
    " [0.         0.         0.         1.        ]\n",
    " [0.00272284 0.99727716 0.         0.        ]]\n",
    "```\n",
    "$B=$       \n",
    "```python\n",
    "[[0.2434762  0.25247178 0.24800145 0.25605057]\n",
    " [0.24727716 0.23681872 0.34909315 0.16681097]\n",
    " [0.28462222 0.23058695 0.20782446 0.27696637]\n",
    " [0.1857911  0.26246354 0.29707437 0.25467098]]\n",
    "```\n",
    "\n",
    "Note that each row sums to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Decoding using Viterbi algorithm\n",
    "\n",
    "It is not always easy to find the coding and non-coding regions of a genome. We would like to automatically annotate the genome, that is to say to find **the most probable sequence of hidden states** which made it possible to generate the observation sequence.\n",
    "\n",
    "### Reminders on the Viterbi algorithm (1967):\n",
    "\n",
    "- It is used to estimate the most probable sequence of states given the observations and the model.\n",
    "- It can be used to approximate the probability of observing the sequence given the model.\n",
    "\n",
    "It uses two recursion variables:\n",
    "\n",
    "probability: $\\delta_{i}(t) = \\log \\max_{s_1^{t-1}} P(s_{1}^{t-1}, s_t = i, y_1^t)$\n",
    "\n",
    "backtrack: $\\Psi_{j}(t) = \\arg\\max_{i \\in \\Sigma} \\delta_{i}(t-1) a_{ij}$\n",
    "\n",
    "1\\. Initialization (indices starting at 0):\n",
    "\n",
    "$$\\begin{array}{ccccccccc} \n",
    "\\delta_{i} (0) &=& \\log \\pi_{i} +\\log b_{i} (x_{0}) \\\\ \n",
    "\\Psi_{i}(0) &=& -1\n",
    "\\end{array}$$\n",
    "Note: We initialize the first bactracking variable $\\Psi_i(0)$ to $-1$ as this variable should not be used ($-1$ does not correspond to a state).\n",
    " \n",
    "2\\. Recursion: \n",
    "\n",
    "$$ \\begin{array}{ccccccccc} \n",
    "\\delta_{j} (t) &=&\n",
    "\\displaystyle \\left[\\max_{i} \\delta_{i}(t-1) + \\log a_{ij}\\right] + \\log b_{j}(x_{t}) \\\\ \n",
    "\\Psi_{j}(t) &=&\n",
    "\\displaystyle \\arg\\max_{i\\in [1,\\ N]} \\delta_{i}(t-1) + \\log a_{ij} \\end{array}$$\n",
    "\n",
    "3\\. Terminaison (with indices at {$T-1$} in python) \n",
    "\n",
    "$$S^{\\star} = \\max_{i} \\delta_{i}(T-1)$$\n",
    "\n",
    "4\\. Path \n",
    "$$\\begin{array}{ccccccccc} s_{T-1}^{\\star} & = &\\displaystyle \\arg\\max_{i} \\delta_{i}(T-1) \\\\ s_{t}^{\\star} & = & \\displaystyle \\Psi_{t+1}(s_{t+1}^{\\star}) \\end{array}$$\n",
    "\n",
    "The estimate of $\\log p (x_0^{T-1} \\mid \\lambda)$ is obtained by finding the greatest probability in the last column of $\\delta$. \n",
    "\n",
    "Write down the algorithm of the `viterbi (x, Pi, A, B)` method:\n",
    "\n",
    "**Note**: if you encounter problem with $0$ cells giving infinite log values, you can try to add a very low value $\\epsilon$ to all the cells of the transition matrix $a_{ij}$ and for the emission probabilities $b_j$  (something like $\\epsilon=10^{-10}$). Be carefull to renormalize the rows of $a$ and each of the $b_j$ to sum to 1 afterward.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(allx,Pi,A,B):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    allx : array (T,)\n",
    "        Sequence d'observations.\n",
    "    Pi: array, (K,)\n",
    "        Distribution de probabilite initiale\n",
    "    A : array (K, K)\n",
    "        Matrice de transition\n",
    "    B : array (K, M)\n",
    "        Matrice d'emission matrix\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    ## initialisation\n",
    "    psi = np.zeros((len(A), len(allx))) # A = N\n",
    "    psi[:,0]= -1\n",
    "    delta = np.zeros((len(A), len(allx))) #Initializing delta\n",
    "    \n",
    "    epsilon = 10**-10\n",
    "    A_eps = np.copy(A)\n",
    "    A_eps[A_eps == 0] = epsilon\n",
    "    \n",
    "    B_eps = np.copy(B)\n",
    "    B_eps[B_eps == 0] = epsilon\n",
    "    \n",
    "    for x in range(len(allx)):\n",
    "        if x == 0:\n",
    "            #print(Pi_eps, np.log(Pi_eps))\n",
    "            first_run = Pi * B[:,allx[x]]\n",
    "            delta[:,x] = Pi\n",
    "            first_run[first_run == 0] = epsilon\n",
    "            first_run = np.log(first_run)\n",
    "        elif x==1:\n",
    "            last_max_val = first_run\n",
    "            second_run = np.max(first_run, axis = 0) + np.log(A_eps[np.argmax(first_run, axis = 0),:] * B_eps[:, allx[x]])\n",
    "            delta[:, x] = np.max(first_run, axis = 0)\n",
    "            index = np.argmax(first_run, axis = 0)\n",
    "            psi[:, x] = index\n",
    "            delta[:, x] = second_run\n",
    "        else:\n",
    "            temp = np.zeros((len(A), len(A)), dtype = float)\n",
    "            last_max_val = delta[:, x-1]\n",
    "            for i, col in enumerate(last_max_val):    \n",
    "                for j in range(len(A)):\n",
    "                    temp[j, i] = last_max_val[i] + np.log(A_eps[i,j] * B_eps[j, allx[x]])\n",
    "            index = np.argmax(temp, axis = 1)\n",
    "            #print(index)\n",
    "            psi[:, x] = index\n",
    "            delta[:, x] = np.take_along_axis(temp.T, index[np.newaxis, :], axis = 0)\n",
    "            \n",
    "    begin_q = np.argmax(delta[:,len(allx)-1])\n",
    "    path = get_path(psi,begin_q)\n",
    "    \n",
    "    return path\n",
    "\n",
    "def get_path(psi, begin_q):\n",
    "    \"\"\"\n",
    "    From the matrix of psi values and the value S*, get the path of maximal values\n",
    "    Parameters\n",
    "    ----------\n",
    "    psi: array (K,T)\n",
    "    begin_q = int - value between 0 and K-1\n",
    "    \"\"\"\n",
    "    ##Your code here\n",
    "    path = np.zeros(psi.shape[1])\n",
    "    path[-1] = begin_q\n",
    "    for i in range(psi.shape[1]-2, -1, -1):\n",
    "        path[i] = psi[path[i+1].astype(int), i+1]\n",
    "        \n",
    "    return path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a small sequence if you want to test your Viterbi code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Small code to test viterbi result\n",
    "test_seq = \"CGTGATATCATCAGGGCAGACCGGTTACATCCCCCTAACAAGCTGTTTAAAGAGAAATACTATCATGACGGACAAATTGACCTCCCTTCGTCAGTACACCACCGTAGTGGCCGACACTGGGGACATCGCGGCAATGAAGCTGTATCAACCGCAGGATGCCACAACCAACCCTTCTCTCATTCTTAACGCAGCGCAGATTCCGGAATATCGTAAGTTGATTGATGATGCTGTCGCCTGGGCGAAACAGCAGAGCAACGATCGCGCGCAGCAGATCGTGGACGCGACCGACAAACTGGCAGT\"\n",
    "dDNA = {\"A\": 0 , \"C\": 1 , \"G\": 2, \"T\": 3}\n",
    "test_seqi = np.array([dDNA[c] for c in test_seq])\n",
    "\n",
    "path = viterbi(test_seqi, Pi, A, B)\n",
    "\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You should find the following state sequence after running Viterbi:\n",
    "\n",
    "```python\n",
    "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2,\n",
    "       3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3,\n",
    "       1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1,\n",
    "       2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2,\n",
    "       3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3,\n",
    "       1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1,\n",
    "       2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2,\n",
    "       3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3,\n",
    "       1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1,\n",
    "       2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2,\n",
    "       3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3,\n",
    "       1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2])\n",
    "```\n",
    "\n",
    "You can now predict the states on your test sequences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_states = viterbi(genome_test, Pi, A, B)\n",
    "\n",
    "test_annotation = annotation_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display\n",
    "\n",
    "Lets simplify the annotation to two categories: \n",
    " - **coding** ($1$) \n",
    " - and **non coding** ($0$).\n",
    "\n",
    "We can simply do that with a reallocation of the matrix of predictions (note that intergenic is state 0).\n",
    "\n",
    "```python\n",
    "predicted_states[predicted_states != 0]=1 \n",
    "test_annotation[test_annotation != 0]=1\n",
    "```\n",
    "Then we will print for each genomic position if it is a coding or a non coding position using the true annotations and add the prediction on that.\n",
    "```python\n",
    "fig, ax = plt.subplots(figsize=(15,2))\n",
    "ax.plot(test_annotation, label=\"annotation\", lw=3, color=\"black\", alpha=.4)\n",
    "ax.plot(predicted_states, label=\"prediction\", ls=\"--\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Display here\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "predicted_states[predicted_states != 0]=1 \n",
    "test_annotation[test_annotation != 0]=1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,2))\n",
    "ax.plot(test_annotation, label=\"annotation\", lw=3, color=\"black\", alpha=.4)\n",
    "ax.plot(predicted_states, label=\"prediction\", ls=\"--\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can consider a part of the genome, plot the values between the positions 100,000 and 200,000. Comment on the quality of the prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,2))\n",
    "ax.plot(test_annotation[100000:200000], label=\"annotation\", lw=3, color=\"black\", alpha=.4)\n",
    "ax.plot(predicted_states[100000:200000], label=\"prediction\", ls=\"--\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_your comment here, which state are well predicted? Do we overpredict sometimes? Are there non coding regions predicted as coding? Why would the model predict that? Conversely, are there coding regions that are predicted as intergenic?_ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The model predict better for non-coding area. The graph shown a big portion that is actually non-coding area, but predicted as coding (between position 55k to 60k.)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 :  Performance Evaluation\n",
    "\n",
    "Using predictions and annotations of the genome, compute the confusion matrix. \n",
    "\n",
    "<div  align=\"left\"><img src=\"confusion.png\" width=\"200\"></div>\n",
    "\n",
    "In other words, we have: \n",
    "- TP = True Positives, coding regions that are correctly predicted,\n",
    "- FP = False Positives, intergenic regions predicted as coding regions,\n",
    "- TN = True Negatives, intergenic regions correctly predicted,\n",
    "- FN = False Negatives, coding regions predicted as intergenic.\n",
    "\n",
    "**non coding** state has index $0$, the other states ($1,2,3$) are **coding** states.\n",
    "\n",
    "![](conf2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def create_confusion_matrix(true_sequence, predicted_sequence):\n",
    "    ## your code here\n",
    "    mat_conf = confusion_matrix(true_sequence, predicted_sequence)\n",
    "    return mat_conf\n",
    "\n",
    "###Display the confusion matrix\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "mat_conf=create_confusion_matrix(annotation_test, predicted_states)\n",
    "plt.imshow(mat_conf)\n",
    "plt.colorbar()\n",
    "ax = plt.gca();\n",
    "\n",
    "# Major ticks\n",
    "ax.set_xticks(np.arange(0, 2, 1));\n",
    "ax.set_yticks(np.arange(0, 2, 1));\n",
    "\n",
    "# Labels for major ticks\n",
    "ax.set_xticklabels(['coding','intergenic']);\n",
    "ax.set_yticklabels(['Predicted coding','Predicted intergenic']);\n",
    "\n",
    "print(mat_conf)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give an interpretation of the results, can we use this model to predict the position of the genes in the genome?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The TN and TP are high, but the FP and FN are also high. Meaning the model predict a large part of intergenic area as a coding area and smaller part of coding area predicted as intergenic area. If we are doing prediction using this model, then we will have a low accuracy score. The model is definitely need to be improved. </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: Generating new sequences \n",
    "\n",
    "Using the model estimated $ \\Theta=\\{Pi,A,B\\}$, specify a function ` create_seq(N,Pi,A,B) ` that, given a sequence length `N` would return:\n",
    "- a sequence of hidden states\n",
    "- a sequence of observations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_seq(N,Pi,A,B):\n",
    "    '''\n",
    "    Return a sequence of N hidden states using Pi and A\n",
    "    and for each hidden state return an observation using B\n",
    "    '''\n",
    "    ## your code here\n",
    "    new_seq = np.zeros(N, dtype = int)\n",
    "    new_obs = np.zeros(N, dtype = int)\n",
    "    prob = np.zeros((N,4), dtype = float)\n",
    "    for i in range(N):\n",
    "        if i == 0:\n",
    "            temp = np.matmul(Pi, A)\n",
    "            new_seq[i] = np.argmax(temp, axis = 0)\n",
    "            new_obs[i] = np.argmax(B[:,new_seq[i]], axis = 0)\n",
    "            prob[i] = temp[new_seq[i]]\n",
    "        else:\n",
    "            temp = np.matmul(prob[i-1], A)\n",
    "            new_seq[i] = np.argmax(temp, axis = 0)\n",
    "            new_obs[i] = np.argmax(B[:,new_seq[i]], axis = 0)\n",
    "            prob[i] = temp\n",
    "    return new_obs, new_seq\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_observation, new_hidden_states = create_seq(500, Pi, A, B)\n",
    "print(new_observation)\n",
    "print(new_hidden_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6: Improving the model\n",
    "\n",
    "Now let's assess if we can improve our prediction by incorporating an addtional layer of information in the model. We will take into account the gene boundaries by building a model that explicitly detects start codon and stop codon.\n",
    "We now want to integrate the additional information that says that a gene \"always\" begins with a start codon and \"always\" ends with a stop codon with the transition graph below.\n",
    "\n",
    "The model now has 12 hidden states.\n",
    "![](modele2.png)\n",
    "\n",
    "- Write the corresponding transition matrix, setting the transition probabilities between letters for stop codons to 0.5.\n",
    "\n",
    "\n",
    "- Adapt the emissions matrix for all states of the model. You can reuse matrix B, calculated previously. The states corresponding to the stop codons will emit only one letter with a probability $ 1 $.\n",
    "For the start codon, we know that the proportions are as follows:\n",
    "    - ATG : 83%, \n",
    "    - GTG: 14%,     \n",
    "    - TTG: 3%\n",
    "\n",
    "```python\n",
    "Pi2 = np.array(   [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ])  ##again, we start in an intergenic region\n",
    "A2 =  np.array([[1-a, a, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ],\n",
    "                [0  , 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0 ],\n",
    "                  ... ])\n",
    "B2 = ...\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assess the performances of the new model by comparing it to the first model on `genome_test`. \n",
    "\n",
    "```\n",
    "predicted_states2=viterbi(genome_test,Pi2,A2,B2)\n",
    "predicted_states2[predicted_states2!=0]=1 \n",
    "```\n",
    "\n",
    "```python\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,2))\n",
    "ax.plot(annotation_test, label=\"annotation\", lw=3, color=\"black\", alpha=.4)\n",
    "ax.plot(etat_predits, label=\"prediction model1\", ls=\"--\")\n",
    "ax.plot(etat_predits2, label=\"prediction model2\", ls=\"--\")\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "Compute the confusion matrix with those new predictions and comment the results. Is it better than the previous one?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Matrix B: how many A-C-T-G in start codon, C0, C1, C2, and stop codon (on each hidden state)\n",
    "#(A=0, C=1, G=2, T=3)\n",
    "def create_matrix(allx, allq, N, K):\n",
    "    A_temp = np.zeros((N, N)) \n",
    "    B_temp = np.zeros((N, K))\n",
    "    \n",
    "    for i in range(len(allq)-1):\n",
    "        if allq[i] == 0: #from intergene\n",
    "            if allq[i+1] == 0: #to intergene\n",
    "                A_temp[0,0] += 1 #intergene to itself\n",
    "            else: #to other than intergene\n",
    "                A_temp[0,1] += 1 # intergene to start codon A T G\n",
    "                A_temp[1,2] += 1 # start codon A T G to T\n",
    "                A_temp[2,3] += 1 # start codon T to G\n",
    "                A_temp[3,4] += 1 # G to C0\n",
    "        else: #not from intergene\n",
    "            if allq[i+1] != 0: #destination other than intergene\n",
    "                A_temp[allq[i]+3, allq[i+1]+3] += 1 #C0 to C1, C1 to C2, C2 to C0\n",
    "            else:  #C2 to Stop codon - Intergene section\n",
    "                A_temp[6,7] += 1 #C2 to stop codon T\n",
    "                if allq[i+1] == 2: #check the next observation \n",
    "                    #A_temp[7,8] += 1 #stop codon T to G\n",
    "                    #A_temp[8,10] += 1 #stop codon G to A\n",
    "                    A_temp[10,0] += 1 #stop codon A to intergene\n",
    "                else:\n",
    "                    #print(allq[i+1])\n",
    "                    #A_temp[7,9]+=1 #stop codon T to A\n",
    "                    if allq[i+2] <= len(allq)-2 and allq[i+2] == 0:\n",
    "                        #A_temp[9,10] += 1 #stop codon A to A\n",
    "                        A_temp[10,0] += 1 #stop codon A to intergene\n",
    "                    else:\n",
    "                        #A_temp[9,11] += 1 #stop codon A to G\n",
    "                        A_temp[11,0] += 1 #stop codon G to intergene\n",
    "    A_temp[7,8] = 0.5\n",
    "    A_temp[8,10] = 1\n",
    "    \n",
    "    A_temp[7,9] = 0.5\n",
    "    A_temp[9,10] = 0.5\n",
    "    A_temp[9,11] = 0.5\n",
    "        \n",
    "    \n",
    "    #(A=0, C=1, G=2, T=3)\n",
    "    B_count = np.zeros((N, K)) \n",
    "    for i, val_i in enumerate(allx):\n",
    "        val_j = allq[i]\n",
    "        B_count[val_j, val_i] += 1\n",
    "            \n",
    "    B_count = normalize_matrix(B_count)\n",
    "    \n",
    "    B_temp[0,:] = B_count[0,:] #intergene\n",
    "    B_temp[4,:] = B_count[1,:] #C0\n",
    "    B_temp[5,:] = B_count[2,:] #C1\n",
    "    B_temp[6,:] = B_count[3,:] #C2\n",
    "    \n",
    "    B_temp[1,0] = 0.83 #start codon A\n",
    "    B_temp[1,2] = 0.14 #start codon G\n",
    "    B_temp[1,3] = 0.03 #start codon T\n",
    "    \n",
    "    B_temp[2,3] = 1 #start codon T\n",
    "    B_temp[3,2] = 1 #start codon G\n",
    "    \n",
    "    B_temp[7,3] = 1 #stop codon T\n",
    "    \n",
    "    B_temp[8,2] = 1 #stop codon G\n",
    "    B_temp[9,0] = 1 #stop codon A\n",
    "    \n",
    "    B_temp[10,0] = 1 #stop codon A\n",
    "    B_temp[11,2] = 1 #stop codon G\n",
    "    \n",
    "    #print(B_temp)\n",
    "    \n",
    "    B = B_temp\n",
    "    \n",
    "    A = normalize_matrix(A_temp)\n",
    "    #B = normalize_matrix(B_temp.T)\n",
    "    return A, B\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from intergene to start\n",
    "#print(genome_train[180:196])\n",
    "#print(annotation_train[180:196])\n",
    "\n",
    "#from codon to stop\n",
    "#print(genome_train[250:265])\n",
    "#print(annotation_train[250:265])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pi2 = np.array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ])\n",
    "A2, B2 = create_matrix(genome_train, annotation_train, 12, 4)\n",
    "print(A2)        \n",
    "print(B2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_states2 = viterbi(genome_test, Pi2, A2, B2)\n",
    "test_annotation2 = annotation_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_states2[predicted_states2 != 0]=1 \n",
    "test_annotation2[test_annotation2 != 0]=1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,2))\n",
    "ax.plot(test_annotation2, label=\"annotation\", lw=3, color=\"black\", alpha=.4)\n",
    "ax.plot(predicted_states2, label=\"prediction\", ls=\"--\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_conf=create_confusion_matrix(test_annotation2, predicted_states2)\n",
    "plt.imshow(mat_conf)\n",
    "plt.colorbar()\n",
    "ax = plt.gca();\n",
    "\n",
    "# Major ticks\n",
    "ax.set_xticks(np.arange(0, 2, 1));\n",
    "ax.set_yticks(np.arange(0, 2, 1));\n",
    "\n",
    "# Labels for major ticks\n",
    "ax.set_xticklabels(['coding','intergenic']);\n",
    "ax.set_yticklabels(['Predicted coding','Predicted intergenic']);\n",
    "\n",
    "print(mat_conf)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7: Integrating reverse strand\n",
    "\n",
    "We now want to add the information about the genes that could come from the complementary strand. \n",
    "\n",
    "If you are unsure about what the forward and complementary strands means, you can check the videos above and the following video that summarizes it (the end about gene order is not important):\n",
    "https://youtu.be/JC6ew2xnJBA\n",
    "\n",
    "In summary for a sequence of DNA that is written as `GCGATGCGTTGTAAACGCGATCAGCGCAT`, we have in fact two sequences, one for each strand:\n",
    "```\n",
    "                 x--------->\n",
    "          5'  GCGATGCGTTGTAAACGCGATCAGCGCATGGG  3'   forward (or plus) strand\n",
    "              ||||||||||||||||||||||||||||||||\n",
    "          3'  CGCTACGCAACATTTGCGCTAGTCGCGTACCC  5'   complementary (or minus) strand\n",
    "                                  <-------x\n",
    "```\n",
    "\n",
    "On the example above, there are two genes, one on the forward strand and one on the complementary strand. Because the genes are annotated using only forward strand, we will need to detect the gene information using the _reverse complementary sequence_. \n",
    "\n",
    "In other words, we will add states for genes on the minus strand in the following way:\n",
    "- we enter a minus gene with either `TCA`, `CTA`, `TTA` (reverse complementary of the stop codons: `TGA`, `TAG`, `TAA`)\n",
    "- we leave a gene with `[C]A[TCA]` (reverse complementary of a start codon) \n",
    "\n",
    "\n",
    "\n",
    "The corresponding transition graph is as follow (You can see that this model has 22 states, numbered in orange):\n",
    "\n",
    "![](model_2_strands.png)\n",
    "\n",
    "A perfect annotation for our small example sequence woul be:\n",
    "```\n",
    "                 x---------->\n",
    "          5'  GCGATGCGTTGATAAACGCGATCAGCGCATGGG  3'   forward (or plus) strand\n",
    "              |||||||||||||||||||||||||||||||||\n",
    "          3'  CGCTACGCAACTATTTGCGCTAGTCGCGTACCC  5'   complementary (or minus) strand\n",
    "                                   <-------x\n",
    "                            1      111111222          state number\n",
    "              000123456456790000000236789012000       \n",
    "                                   \n",
    "```\n",
    "\n",
    "To implement such a model, you will have to\n",
    " - deduce from the observation matrices for codons a second one for the codons that are seen on the reverse strand. You can make the hypothesis that the codon distribution is the same on both strands.\n",
    " - encode the transition matrix for observing genes on the reverse strand, starting from a reverse codon stop and ending with a codon start.\n",
    "\n",
    "\n",
    "Implement a third model `Pi3, A3, B3` that would take into account the reverse strand and evaluate its performances with respect to model 1 and model 2.\n",
    "\n",
    "\n",
    "**Note**: Be careful with the evaluation, the annotation given only provides the genes that are on the forward strand (the genes on the reverse strand are annotated as intergenic). If we use the numbering provided in the figure, we would do something like:\n",
    "\n",
    "```python\n",
    "predicted_states[predicted_states > 11]=0 #reverse strand genes as negatives\n",
    "predicted_states[predicted_states != 0]=1 #forward strand genes as positives\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8: Implementing the forward-backward algorithm (optional)\n",
    "\n",
    "Using the information presented in the lecture, implement an EM estimation of the parameter based on the forward-backward algorithm. \n",
    "\n",
    "1.  Write a function for the forward algorithm\n",
    "2.  Write a function for the backward algorithm\n",
    "3.  Deduce a function to compute the smoothing probabilities\n",
    "4.  Write the EM algorithm and compare the estimation results with the viterbi based method.\n",
    "\n",
    "Did you encounted any problem while implementing this algorithm? Detail and comment each step of your analysis\n",
    "\n",
    "**Note:** The forward and backward values decrease exponentially fast with the length of the sequence, leading to numerical issues. **You will need to integrate rescaling factors for the probabilities (as described in the lecture)**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
